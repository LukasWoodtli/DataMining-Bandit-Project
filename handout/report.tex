\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}
\usepackage{listings}

%\lstset{
%breaklines=true,
%breakatwhitespace=true
%}
\lstset{basicstyle=\small}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\setcounter{secnumdepth}{1}

\renewcommand{\familydefault}{\sfdefault}

\title{Data Mining: Learning from Large Data Sets - Fall Semester 2015}
\author{mwurm@student.ethz.ch\\ merkim@student.ethz.ch\\ lwoodtli@student.ethz.ch\\}
\date{\today}

\begin{document}
\maketitle

\section*{Extracting Representative Elements} 

\subsection{Problem Description}
The goal of this project was to learn a policy that explores and exploits available user choices for yahoo news articles. This was done in order to learn user preferences and recommend other relevant articles to the users.

\subsection{Approach of the Team}
In this project the team decided that the first approach to follow was to implement the LinUCB algorithm which is described in the lecture slides. This version includes the user features into its evaluation. This should not be confused with the version in the original paper [Li et al WWW'10],  which uses the articles instead.

The easier $\epsilon$-greedy approach was declined from the beginning, since it was assumed that it will never be good enough to surpass the baseline hard.
The algorithm was then implemented iteratively until it worked properly.
Performance was an important factor to consider. While the first naive implementation did not terminate because of a timeout, the program ran through fast enough after several optimizations. It was necessary to export as much code as possible to the update function instead of executing it for each step in the recommend function. Especially calculating the matrix inverse in the update function has a significant impact onto the performance.

After the algorithm was optimized to run in a reasonable time it needed to be optimized to provide a good result. The main parameter for optimizing the prediction result was $\alpha$. The ideal value for the parameter was determined mainly by trial. A problem was, that the local test template did not give usable feedback and the only way to get feedback was through an online-submit. Since the basic functionality of the LinUCB algorithm was actually the same for multiple groups, we divided the cross testing for alpha to end up with a value of around 0.27.


\subsection{Environment}
In contrast to the previous projects, the code for this one was not executed within a map-reduce environment. There was only a single policy file, in which the heuristic to predict the user click behaviour had to be implemented.\\
As training data, log data of yahoo was available. Training the model on real log data, it is of course not possible to get a feedback for each prediction. The reason for this is that yahoo might have displayed another article. Like this we could have gotten a feedback for the other prediction but not for the one we made.


\subsection{Possible Improvements}
A possible improvement would be the implementation of the Hybrid LinUCB algorithm. It has the same runtime complexity as the standard LinUCB algorithm, but it captures both separate and shared effects.

\subsection{Conclusion}
This project shows that sometimes a proper tweaking of the parameter variables might be equally helpful as using an overall more complex algorithm. It reveals that proper proposals can be made, using a rather easy algorithm.
\end{document} 
